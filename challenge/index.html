
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>LTVRR Challenge</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Long Tail Visual Relationship Recognition Challenge</br> 
                <small>
                    Co-organized with the <a href="https://sites.google.com/view/l3d-ivu-2023/overview"></a>L3D-IVU Workshop</a> CVPR 2023
                </small>
            </h2>
        </div>
        <br>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="#servers">
                          Submission Servers
                        </a>
                    </li>
                    <li>
                        <a href="#dataset">
                          Dataset Download
                        </a>
                    </li>
                    <li>
                        <a href="#code">
                          Starter Code
                        </a>
                    </li>
                </ul>   
            </div>
        </div>
        <br>

        <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4>
                        <b>Submission Servers are open!</b>
                    </h4>
                    <image src="img/examples.png" class="img-responsive" alt="overview"></image>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    About
                </h3>
                <p class="text-justify">
                    <!-- The challenge is part of Workshop on Learning with Limited Labelled Data for Image and Video Understanding, held at CVPR 2022. -->
                    The main goal of the challenge is to evaluate and benchmark new and better methods for GQA-LT and VG8K-LT benchmarks proposed in <a href="https://ltvrr.github.io/">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary</a>.
                    The main task for the challenge is Visual Relationship Recognition. Both the benchmarks, GQA-LT and VG8K-LT, are long-tailed created out of GQA and VG datasets.
                    With the presence of bounding boxes during both training and test time, the participants are required to submit a csv file containing the detection results of subject/object 
                    and the relation classes in the corresponding triplet.
                </p>
                <hr>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Task & Metrics Used
                </h3>
                <p class="text-justify">
                    The main task for both of these benchmarks is to predict relationship and at the same time subject/object associated with the triplet. During training and evaluation,
                    the ground-truth bounding boxes are already provided and hence the main aim is for the classification of sbj/obj and the rel associated with them.
                    The main metric used for the challenge is <i>average per-class accuracy</i>, which is the accuracy of each class calculated separately, then average.
                    The average per-class accuracy is a commonly used metric in the long-tail literature.
                    We report the results on the subject, object, and relation separately of an (S,R,O) triplet on GQA-LT and VG8K-LT datasets. We evaluate
                    the models using the average per-class accuracy across several frequency bands chosen based on frequency percentiles for GQA-LT and VG8K-LT, which can be seen below:
                </p>
                <center>
                    <image src="img/gqa_split.png" class="img-responsive" alt="overview"></image>
                    <figcaption>Table 1 - GQA-LT splits</figcaption>
                </center>
                <br>
                <center>
                    <image src="img/vg8k_split.png" class="img-responsive" alt="overview"></image>
                    <figcaption>Table 2 - VG8K-LT splits</figcaption>
                </center>
                <br>
                <p class="text-justify">
                    Hence the metrics <i>rel_many, rel_med, rel_few, rel_all_per_class</i> are calculated by taking into account Table 1 & 2 for relations, and <i>sbj_obj_many, sbj_obj_med, sbj_obj_few, sbj_obj_all_per_class</i>
                    are calculated by taking into account Table 1 & 2 for sbj/obj. The other two metrics <i>rel_all_per_example</i> and <i>sbj_obj_all_per_example</i> are the standard metrics 
                    that just average individual prediction scores that has been used in the literature. You can also find more info about the dataset and the metrics in the <a href='docs/supplementary_document.pdf'>supplementary document</a>.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Subission Format
                </h3>
                <p class="text-justify">
                    The submission is supposed to be a csv file (named <i>rel_detections_gt_boxes_prdcls.csv</i> when the given evaluation code is run on starter code) that is to be renamed 
                    <b>answer.csv</b> and compressed into a .zip file (named <b>submission.zip</b>) before submission. No other format of submission will be accepted.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="servers">
                    Servers
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>Challenge 1:</b> GQA-LT Benchmark: <a href="https://competitions.codalab.org/competitions/34948">Link</a>
                        </li>
                        <li>
                            <b>Challenge 2:</b> VG8K-LT Benchmark: <a href="https://competitions.codalab.org/competitions/34949">Link</a>
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="dataset">
                    Download Dataset
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>GQA-LT:</b> <br> You can download the annotations and splits necessary for the GQA-LT benchmark <a href="https://drive.google.com/file/d/1ypmMOq2TkZyLNVuU9agHS7_QcsfTtBmn/view?usp=sharing">here</a>.
                            You should see a 'gvqa' folder once unzipped. It contains seed folder called 'seed0' that contains .json annotations suited for the dataloader used in our implementations. Also, download GQA images from <a href="https://cs.stanford.edu/people/dorarad/gqa/download.html">here</a>
                        </li>
                        <li>
                            <b>VG8K-LT:</b> <br> You can download the annotations and splits necessary for the VG8K-LT benchmark <a href="https://drive.google.com/file/d/1S8WNnK0zt8SDAGntkCiRDfJ8rZOR3Pgx/view?usp=sharing">here</a>.
                            You should see a 'vg8k' folder once unzipped. It contains seed folder called 'seed3' that contains .json annotations suited for the dataloader used in our implementations. Also, download VG images from <a href="https://visualgenome.org/api/v0/api_home.html">here</a>
                        </li>
                    </ul>

                    Further instructions about the dataset placing and also dataloader can be found in tthe README of starter code <a href="https://github.com/Vision-CAIR/LTVRR/tree/challenge-submission">here</a>.

                </p>
                <hr>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="code">
                    Code
                </h3>
                <p class="text-justify">
                    You can find the starter code that can be used for submissions <a href="https://github.com/Vision-CAIR/LTVRR/tree/ltvrd-challenge-2023">here</a>. <i>Please follow the code as presented in the branch 'challenge-submission' for 
                    creating an output in the format that is required for the submission</i>. For running the baseline models you can follows the README 
                    instructions. You can also download some of the pre-trained models present there. After running the test file (here <i>test_net_rel.py</i>), the csv file to be used for submission would be named <i>rel_detections_gt_boxes_prdcls.csv</i>, 
                    which to be renamed according to rules mentioned above. For any query regarding this, please feel free to contact the people mentioned below.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Important Dates
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>16th February, 2023:</b> The challenge portal for submissions for both benchmarks (GQA-LT & VG8K-LT) open up.
                        </li>
                        <li>
                            <b>15th May, 2023:</b> The challenge deadline for both the benchmarks.
                        </li>
                    </ul>
                
                    Workshop: 19th June, 2023
                </p>
                <hr>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Contact
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <a href="mailto:mohamed.elhoseiny@kaust.edu.sa">mohamed.elhoseiny@kaust.edu.sa</a>
                        </li>
                        <li>
                            <a href="mailto:goel.arushi@gmail.com">goel.arushi@gmail.com</a>
                        </li>
                        <li>
                            <a href="mailto:aagarwal@ma.iitr.ac.in">aagarwal@ma.iitr.ac.in</a>
                        </li>
                        <li>
                            <a href="mailto:jun.chen@kaust.edu.sa">jun.chen@kaust.edu.sa</a>
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>
            

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Organizers
                </h3>
                <p class="text-justify">
                    <ul>
                         <li>
                            Mohamed Elhoseiny,  KAUST
                        </li>
                        <li>
                            Arushi Goel, University of Edinburgh
                        </li>
                        <li>
                            Aniket Agarwal,  KAUST Intern/ IIT Roorkee
                        </li>
                        <li>
                            Jun Chen, KAUST 
                        </li>
 
                    </ul>
                </p>
                <hr>
            </div>
        </div>
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{ltvrr2021,
    title={Exploring Long Tail Visual Relationship Recognition with Large Vocabulary},
    author={Abdelkarim, Sherif and Agarwal, Aniket and Achlioptas, Panos and Chen, Jun and Huang, Jiaji and Li, Boyang and Church, Kenneth and Elhoseiny, Mohamed},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    year={2021}
    }
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
